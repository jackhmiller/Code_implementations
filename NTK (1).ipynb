{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7fc6a9ac",
   "metadata": {},
   "source": [
    "# Neural-Tangent-Kernel\n",
    "A python implementation of the neural tangent kernel (NTK)\n",
    "\n",
    "Original NTK paper: https://arxiv.org/abs/1806.07572\n",
    "Blog post: https://jackhmiller.github.io/My-DS-Blog/2021/10/02/NTK.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3526b7-c4ce-4a05-b340-7c42727fcb24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import optim, nn\n",
    "import copy\n",
    "import warnings\n",
    "from pylab import *\n",
    "import imageio\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654eae1e-5475-41df-be0e-c20156c27bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "rcParams['figure.figsize'] = 12,9\n",
    "rcParams['axes.grid'] = True\n",
    "rcParams['font.size'] = 20\n",
    "rcParams['lines.linewidth'] = 3\n",
    "DEFAULT_COLORS = rcParams['axes.prop_cycle'].by_key()['color']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d8a80b-3803-4deb-a803-0e9ea81cdcce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install \"jax[cpu]===0.3.14\" -f https://whls.blob.core.windows.net/unstable/index.html --use-deprecated legacy-resolver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e516427c-d609-4378-9e65-192cda792119",
   "metadata": {},
   "outputs": [],
   "source": [
    "act_dict = {\n",
    "    'relu': nn.ReLU,\n",
    "    'tanh': nn.Tanh\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43d01d4-ee56-4113-a055-5bf18a1f31c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ZeroOutput(nn.Module):\n",
    "    \"\"\"Zero the output of a model by subtracting out a copy of it.\"\"\"\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.init_model = [copy.deepcopy(model).eval()]\n",
    "        \n",
    "        self.model = model\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x) - self.init_model[0](x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fdec37b-6691-46dd-a69e-a98df9ed6302",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Scale(nn.Module):\n",
    "    \"\"\"Scale the output of the model by alpha.\"\"\"\n",
    "    def __init__(self, model, alpha):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.alpha = alpha\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.alpha*self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8526fd2-3aab-4787-9a69-14520c23750c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_net(width,\n",
    "              bias=True,\n",
    "              zero_output=True,\n",
    "              alpha=1,\n",
    "              hidden_layers=1,\n",
    "              act='relu',\n",
    "              **kwargs):\n",
    "    \"\"\"A simple 1d input to 1d output deep ReLU network.\"\"\"\n",
    "    \n",
    "    activation = act_dict[act]\n",
    "    model = nn.Sequential(nn.Linear(1, width, bias=bias),\n",
    "                         activation(),\n",
    "                         *[layer for _ in range(hidden_layers-1)\n",
    "                          for layer in [nn.Linear(width, width, bias=bias), activation()]],\n",
    "                         nn.Linear(width, 1, bias=bias))\n",
    "    if zero_output:\n",
    "        model = ZeroOutput(model)\n",
    "    model = Scale(model, alpha)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a16c02-dd37-4fa6-8ca3-a8e818b5bbed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ntk(model, x):\n",
    "    \"\"\"Calculate the neural tangent kernel of the model on the inputs.\"\"\"\n",
    "    \n",
    "    out = model(x)\n",
    "    p_vec = nn.utils.parameters_to_vector(model.parameters())\n",
    "    p, = p_vec.shape\n",
    "    n, outdim = out.shape\n",
    "    \n",
    "    features = torch.zeros(n, p, requires_grad=False)\n",
    "    \n",
    "    for i in range(n):\n",
    "        model.zero_grad()\n",
    "        out[i].backward(retain_graph=True)\n",
    "        p_grad = torch.tensor([], requires_grad=False)\n",
    "        for p in model.parameters():\n",
    "            p_grad = torch.cat((p_grad, p.grad.reshape(-1)))\n",
    "        features[i, :] = p_grad\n",
    "        \n",
    "    tangent_kernel = features@features.t()\n",
    "    return features, tangent_kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace1f7b1-803f-4245-ba70-6c580bd3757a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gd(model, xdata, ydata,\n",
    "       iters=100,\n",
    "       lr=1e-3,\n",
    "       alpha=1,\n",
    "       eps=1e-10):\n",
    "    \"\"\"Gradient Descent using normalized (depending on alpha) L2 loss of model\"\"\"\n",
    "    opt = optim.SGD(model.parameters(), lr=lr)\n",
    "    losses = []\n",
    "    \n",
    "    litem = -1\n",
    "    t = range(iters)\n",
    "    for i in t:\n",
    "        out = model(xdata)\n",
    "        loss = 1/(alpha**2) * nn.MSELoss()(out, ydata)\n",
    "        litem = loss.item()*(alpha**2)\n",
    "        losses.append(litem)\n",
    "        if litem < eps:\n",
    "            return losses\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b2360f8-985e-4ecf-be39-b4f568271617",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_gd(A, b, x0,\n",
    "             iters=100,\n",
    "              lr=1e-3,\n",
    "              alpha=1,\n",
    "             eps=1e-10):\n",
    "    m, p = A.shape\n",
    "    x = nn.Parameter(x0.clone())\n",
    "    opt = optim.SGD([x], lr=lr)\n",
    "    losses = []\n",
    "    \n",
    "    litem = -1\n",
    "    for i in range(iters):\n",
    "        out = A@(x-x0)\n",
    "        loss = 1/(alpha**2)*nn.MSELoss()(out.speeze(), b)\n",
    "        litem = loss.item()*(alpha**2)\n",
    "        losses.append(litem)\n",
    "        \n",
    "        if litem < eps:\n",
    "            return losses\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        \n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128838f3-ade2-442e-a0a8-f4aa49c55e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = {}\n",
    "budges = {}\n",
    "losses = {}\n",
    "\n",
    "eps = 1e-10\n",
    "iters = 1000\n",
    "steps_per_iter = 1\n",
    "lr = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "000a74f8-5323-461d-ac6a-9cc92c3cd6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "xin = torch.tensor([-3, 0.5]).unsqueeze(1)\n",
    "yin = torch.tensor([2, -1.0]).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4698cb-65fc-46be-8a32-ebd40bbb0588",
   "metadata": {},
   "outputs": [],
   "source": [
    "for m in [10, 100, 1000]:\n",
    "    f = simple_net(width=m, bias=True, alpha=1, zero_output=False, hidden_layers=2)\n",
    "    A0, tk0 = ntk(f, xin)\n",
    "    weights0 = list(f.modules())[4].weight.detach().numpy().copy()\n",
    "    allw0 = nn.utils.parameters_to_vector(f.parameters()).detach().numpy().copy()\n",
    "\n",
    "    imgs = []\n",
    "    imgs2 = []\n",
    "    xvals = [0]\n",
    "    budgevals = [0]\n",
    "    lossvals = []\n",
    "    for i in range(iters):\n",
    "        ls = gd(f, xin, yin, alpha=1, iters=steps_per_iter, lr=lr, progress_bar=False)\n",
    "        lossvals.extend(ls)\n",
    "        weights = list(f.modules())[4].weight.detach().numpy().copy()\n",
    "        allw = nn.utils.parameters_to_vector(f.parameters()).detach().numpy().copy()\n",
    "        budge = norm(allw-allw0)/norm(allw0)\n",
    "        xvals.append((i+1)*steps_per_iter)\n",
    "        budgevals.append(budge)\n",
    "\n",
    "        if ls[-1]<eps:\n",
    "            break\n",
    "    \n",
    "    xs[m] = xvals.copy()\n",
    "    budges[m] = budgevals.copy()\n",
    "    losses[m] = lossvals.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0060ee0c-8388-4bb2-b6d7-b0f4880bf95e",
   "metadata": {},
   "outputs": [],
   "source": [
    "title(f\"Relative change in norm of weights from initialization\")\n",
    "for m in sorted(xs.keys()):\n",
    "    plot(xs[m], budges[m], label=f\"Width {m}\")\n",
    "xlabel(\"Step (n)\")\n",
    "ylabel(r\"$\\frac{\\Vert w(n) -  w(0) \\Vert}{\\Vert w(0) \\Vert}$\")\n",
    "legend()\n",
    "show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecdf64b0-c5a8-4864-873f-4167c5a0cddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "figure()\n",
    "title(f\"Training loss\")\n",
    "for m in sorted(xs.keys()):\n",
    "    plot(arange(len(losses[m])), losses[m], label=f\"Width {m}\")\n",
    "xlabel(\"Step\")\n",
    "ylabel(\"Loss\")\n",
    "legend()\n",
    "show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e3e1e9-f678-4747-8b6f-37aa3595a1c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
