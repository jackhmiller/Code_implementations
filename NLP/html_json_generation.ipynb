{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 315,
   "id": "917b884d-b37d-43bb-a825-aa7ecbd39dd4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import html5lib\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import os\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import warnings\n",
    "import json\n",
    "import numpy as np\n",
    "import re\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "458a2553-2ee4-41a8-acfc-6162384ff76f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tables_path = \"Downloads/generated_tables/tables/\"\n",
    "json_path = \"Downloads/generated_tables/metadata/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "985e95a0-1c3b-47d3-820b-d199c4286683",
   "metadata": {},
   "source": [
    "## Solution Overview\n",
    "The task: Given the structured nature of HTML tables and the hierarchical nature of JSON, this task involves understanding and transforming structured data from one format to another.\n",
    "Upon quick inspection I see three possible approaches to the problem, each with pros and cons. \n",
    "1. Rule based - the HTML structure appears identical for all tables, and is not overly complex. A simple python script to automate the task of extracting elements from HTML and converting them to JSON would likely due the job. However, this approach would not scale if the data source was changed from a HTML to say a table embedded in a PDF, or if the table structure was changed dramatically. \n",
    "2. LLM approach- let the LLM do the heavy lifting by providing it with a few examples (few-shot learning) of html-json pairs, and then give it the HTML to product json. THis could be followed by a validation step to ensure the JSON format is valid. \n",
    "3. A third approach is to use a transformer-based encoder-decoder sequence to sequence model to embed the html text from the table and reconstruct it as JSON. We can leverage the json and html pairs to train the model. The encoder is a BERT embedding + an LSTM layer that converts the HTML table to a context vector. The decoder converts the context vector via an LSTM and a fully connected layer to the tokenized json. We can then calculate the loss by comparing the generated tokens to the actual tokens representing the JSON dictionary. While BERT and LSTM layers are a robust solution for handling the structured and variable-length nature of the data involved, this is a bit overkill since we are not dealing with semantic table interpretation where we would need a transformer based model to perform cell entity or column type annotation. This is also the riskiest in terms of being to train a model that can do the task accurately in terms of minimizing a reconstruction error, since certain elements of the json structure might be more important than others. Thus weighing all generated tokens equally in the loss calculation is risky. A better choice of loss might be the jaccard similarity coefficient score. Further, we are relying on the model to generate not only the json data values but the keys and the schema as well. \n",
    "\n",
    "Approach #3 could be improved upon by encoding four components of a table to form its embedding sequence: row headers, column headers, metadata, and context, as opposed to a single fixed length embedding. Serializing vectors of four components into a sequence of vectors as the embedding of the table could likely preserve the local information in the table. We could use these four vectors to customize the loss function by employing multifield evaluation, where each field is evaluated separately against its JSON component and then the weighted sum of the scores is taken. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2435a05f-1084-4b1a-aafa-6f7a86d177ed",
   "metadata": {
    "tags": []
   },
   "source": [
    "### (1. Rule Based)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd584488-4426-44ad-8d03-9b35b0a68667",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "file_name = os.path.join(tables_path, '1_table.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b0b51f-459e-490f-b4bd-674143c9dbf7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_table_id(html_file):\n",
    "    \n",
    "    with open(file_name, 'r') as f:\n",
    "        soup = BeautifulSoup(f, 'html.parser')\n",
    "        text_elements = str(soup.contents[0])\n",
    "    \n",
    "    start_delim = \"<caption>\"\n",
    "    end_delim = \"</caption>\"\n",
    "\n",
    "    pattern = re.escape(start_delim) + r\"(.*?)\" + re.escape(end_delim)\n",
    "    match = re.search(pattern, text_elements)\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80687f92-efa7-47fe-a3df-c11d976dfc07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(html_file):\n",
    "    html1 = pd.read_html(html_file)\n",
    "    df = html1[0]\n",
    "    cols = list(df.columns[1:].values)\n",
    "    df2 = df.set_index(df.columns[0])\n",
    "    rows = list(df2.index.values[:-1])\n",
    "    creation_text = df2.index.values[-1]\n",
    "    creation_date = df2.index.values[-1].split()[0]\n",
    "    content = list(np.concatenate(df2[:-1].values))\n",
    "    \n",
    "    table_id = get_table_id(html_file)\n",
    "    \n",
    "    jsonn = {\n",
    "    \"body\": {\n",
    "        \"content\": concent,\n",
    "        \"headers\": {\n",
    "            \"col\": cols,\n",
    "            \"row\": rows\n",
    "        }\n",
    "    },\n",
    "    \"footer\": {\n",
    "        \"table_creation_date:\": creation_date,\n",
    "        \"text\": creation_text\n",
    "    },\n",
    "    \"header\": {\n",
    "        \"table_id\": table_id.split()[1],\n",
    "        \"text\": table_id\n",
    "    }\n",
    "}\n",
    "    \n",
    "    return json.dumps(jsonn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb75f05-8cc3-4990-aefb-1ebbfa2c0035",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 3. BERT Encoder-Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "id": "23cca3dd-d250-45cc-9930-6c6ebb1df25c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "id": "a9ec84d2-c975-4a17-9f0e-884f80fdfaa3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class HTMLTextExtractionDataset(Dataset):\n",
    "    def __init__(self, html_path, json_path, max_len=128):\n",
    "        self.html_files = os.listdir(html_path)\n",
    "        self.json_files = os.listdir(json_path)\n",
    "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "        self.max_len = max_len\n",
    "        self.data = self.load_data()\n",
    "    \n",
    "    def load_data(self):\n",
    "        data = []\n",
    "        for html_file, json_file in zip(self.html_files, self.json_files):\n",
    "            with open(os.path.join(html_path, html_file), 'r') as f:\n",
    "                soup = BeautifulSoup(f, 'html.parser')\n",
    "                text_elements = soup.table.text\n",
    "            \n",
    "            with open(os.path.join(json_path, json_file), 'r') as f:\n",
    "                json_data = json.load(f)\n",
    "            \n",
    "            data.append((text_elements, json_data))\n",
    "        return data\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        html_table, json_dict =  self.data[idx]\n",
    "        tokens = self.tokenizer.encode(html_table, padding='max_length', max_length=self.max_len, truncation=True)\n",
    "        json_tokens = self.tokenizer.encode(json.dumps(json_dict), padding='max_length', max_length=self.max_len, truncation=True, add_special_tokens=True)\n",
    "        return torch.tensor(tokens), torch.tensor(json_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "id": "dfe2a05d-b831-47cc-bc51-dac4944dd54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = HTMLTextExtractionDataset(html_path, json_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "id": "cda2c1e3-b85d-48a7-926f-f120d4685f07",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataloader = DataLoader(dataset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "id": "3f14c34c-ebc4-4f6a-98a6-75d9ad68f75e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TableToJSONModel(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        super(TableToJSONModel, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "        self.encoder = nn.LSTM(768, hidden_size, batch_first=True)\n",
    "        self.decoder = nn.LSTM(hidden_size, output_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x, attention_mask=None):\n",
    "        bert_output = self.bert(x, attention_mask=attention_mask)\n",
    "        encoder_output, (hidden, cell) = self.encoder(bert_output.last_hidden_state)\n",
    "        decoder_output, _ = self.decoder(encoder_output)\n",
    "        output = self.fc(decoder_output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "id": "4143cccd-a9ce-4010-8534-c4a57bd93372",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def criteria(logits, target):\n",
    "    logits = logits.view(-1, vocab_size)\n",
    "    return F.cross_entropy(logits, target.view(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "id": "dc68d9de-6748-4de7-9179-020c03fb8d4a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vocab_size = len(tokenizer)\n",
    "hidden_dim = 256\n",
    "linear_layer = nn.Linear(hidden_dim, vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76c214a2-76d6-48ae-ab86-dac1ef71570c",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "id": "f47e96b8-d587-4c21-be26-59d5bb80b039",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = TableToJSONModel(hidden_size=hidden_dim, output_size=hidden_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "id": "edb075b4-d788-41f0-9a81-496da1495698",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "n_epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee6a53c1-3d17-46d4-a99b-45ddef1c61d4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for epoch in range(n_epochs):\n",
    "    total_loss = 0\n",
    "    model.train()\n",
    "    for tables, json_dicts in dataloader:\n",
    "        attention_mask = (tables != tokenizer.pad_token_id).long()\n",
    "        outputs = model(tables, attention_mask=attention_mask)\n",
    "        logits = linear_layer(outputs)\n",
    "        loss = criteria(logits, json_dicts)\n",
    "        loss.requires_grad_(True)\n",
    "        total_loss += loss.item()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f'Epoch {epoch+1}/{n_epochs}, Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc6502fc-9146-426d-8eb7-d4fa66c6e5b5",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38906ea8-63ce-4e28-98c2-66980a3dfcf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, dataloader):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for tables, json_dicts in dataloader:\n",
    "            attention_mask = (tables != tokenizer.pad_token_id).long()\n",
    "            outputs = model(tables, attention_mask=attention_mask)\n",
    "            logits = linear_layer(outputs)\n",
    "            loss = criteria(logits, json_dicts)\n",
    "            total_loss += loss.item()\n",
    "    \n",
    "    average_loss = total_loss / len(dataloader)\n",
    "    print(f'Average Loss: {average_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42d3fbc-0fa0-4053-aee7-b3a32bde786a",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(model, dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4452f291-c0e9-40f3-aafe-55ed6ac64b65",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "id": "18e249fa-8b44-403b-8458-6e9463bd73cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def convert_embedding_to_string(outputs):\n",
    "    tokens = []\n",
    "    for output in outputs:\n",
    "        tokens.append(tokenizer.decode(output.argmax(dim=-1)))\n",
    "        \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e24fda1f-3a26-4ec7-8e11-a97c37f11409",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predict_and_decode(model, html_text, tokenizer, max_len=128):\n",
    "    tokens = tokenizer.encode(html_text, padding='max_length', max_length=max_len, truncation=True)\n",
    "    inpt =  torch.tensor(tokens)\n",
    "    attention_mask = (inpt != tokenizer.pad_token_id).long()\n",
    "    outputs = model(inpt, attention_mask=attention_mask)\n",
    "    output_string = convert_embedding_to_string(outputs)\n",
    "    return tokens_to_json(output_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ac87bc-0c64-4976-af31-c5d8801ed09f",
   "metadata": {},
   "outputs": [],
   "source": [
    "html_inference_file = # TODO insert html file name to test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2d085a-8bb8-4034-a13a-0c468a91a40d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(html_inference_file, 'r') as f:\n",
    "    soup = BeautifulSoup(f, 'html.parser')\n",
    "    text_elements = soup.table.text\n",
    "    prediction = predict_and_decode(model, text_elements, tokenizer)\n",
    "    print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed217baf-79de-4e60-8126-b683787ad303",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
