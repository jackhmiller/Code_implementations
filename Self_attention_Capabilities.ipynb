{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "NXGwfjJq84oL"
      },
      "source": [
        "# Self-attention Capabilities\n",
        "\n",
        "This notebook provides a hands-on presentation of examples with actual numbers comparing the attention mechanism used in transformers to 1D convolutional neural networks (CNNs).\n",
        "The examples shown here will all use self-attention.\n",
        "\n",
        "This discussion is designed to follow an overview of the architecture of the attention mechanism used in transformer neural networks,\n",
        "such as Jay Alammar's excellent [Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/) or Yan Xu's [Step by Step into Transformer](https://medium.com/@YanAIx/step-by-step-into-transformer-79531eb2bb84), so that the reader is already familiar the the components of the attention mechanism and how they are connected."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "mUICFDV4tk2-"
      },
      "source": [
        "We also want to take a moment to comment on the limitation of seq2seq models using RNN or LSTM modules.\n",
        "Lilian Weng has some amazing blog posts, and in the [one about attention](https://lilianweng.github.io/posts/2018-06-24-attention/), she shows the following diagram and explains, \"A critical and apparent disadvantage of this fixed-length context vector design is incapability of remembering long sentences.\"\n",
        "As you get longer sequences, not only is it difficult to pack the meaning of all the words into this fixed-lengtch context vector, but it also gets harder for the network to remember the meaning from early words that were pocessed so many iterations ago. \\\n",
        "<img src=\"https://lilianweng.github.io/posts/2018-06-24-attention/encoder-decoder-example.png\" width=750>"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Fp9NVqo5eyen"
      },
      "source": [
        "Before we dive in, let's load the packages we'll be using.  All examples are in Python using PyTorch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0x2c8xIm8u8s"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "hQqUbcdq9d0I"
      },
      "source": [
        "## 1. Pattern matching with CNNs"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "yeNF1gsofJIi"
      },
      "source": [
        "We will discuss convolutional neural networks (CNNs) first."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "UmFq8z_U90Mg"
      },
      "source": [
        "### 1.1 Refresher on CNNs"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "5DdzBkq1-CPw"
      },
      "source": [
        "#### 1.1.1 Using 2D CNNs for image processing"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "icj7NKjCy4AC"
      },
      "source": [
        "We will assume the reader has been exposed to CNNs.  You probably have seen images like this one from [Arden Dertat's Toward Data Science article](https://towardsdatascience.com/applied-deep-learning-part-4-convolutional-neural-networks-584bc134c1e2). \\\n",
        "<img src=\"https://miro.medium.com/v2/resize:fit:828/1*VVvdh-BUKFh2pwDD0kPeRA@2x.gif\" width=600> \\\n",
        "Here, the 3x3 filter in green is shown matching up with all the different positions on the blue image to compute values shown in red on the right."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "pFoqNP9d-OeT"
      },
      "source": [
        "#### 1.1.2 Channels and 2D CNNs"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "7b5TuFB70t4V"
      },
      "source": [
        "It's easy to find descriptions and images like the one above.  For simplicity, these images show a CNN extracting features from an image with only one channel, such as a grey scale image.  One thing those images don't explain is what happens when the image has multiple channels.\n",
        "\n",
        "If an image has multiple channels, such as a typical 3-channel color image with RGB channels, then our filter is a three-dimensional tensor.  A 3x3 filter on a RGB image has shape 3x3x3 and consists of 27 parameters (not counting a bias term).  A 3x3 filter on a 256 channel image has shape 3x3x256 and consists of 2,304 parameters (again without a bias value).\n",
        "\n",
        "Here is an image from [Daphne Cornelisse's freeCodeCamp article](https://www.freecodecamp.org/news/an-intuitive-guide-to-convolutional-neural-networks-260c2de0a050/) that has an animation from [the Stanford cs231n course](https://cs231n.github.io/convolutional-networks/).\n",
        "This animation shows two \"3x3\" filters (which are actually each shape 3x3x3) operating on a 5x5 image with three channels with padding (which acts like a 7x7x3 object).  The fact that there is a separate 3x3 set of filter parameters for each channel is clearly shown.\n",
        "(The example also uses stride 2, which is why the filter jumps two pixels each time it moves right or down.) \\\n",
        "<img src=\"https://cdn-media-1.freecodecamp.org/images/gb08-2i83P5wPzs3SL-vosNb6Iur5kb5ZH43\" width=600>"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "THsQw_lH-mwY"
      },
      "source": [
        "### 1.2 Using 1D CNNs for sequence processing"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "zlGockQL6rbT"
      },
      "source": [
        "Most of the time when explaining multidimension objects, it makes sense to start small and work toward larger numbers of dimensions.\n",
        "In this case, we started with 2D CNNs because of how well their use in computer vision has been documented.\n",
        "\n",
        "Our CNN code in this notebook will be using 1D CNNs operating on sequences.\n",
        "In this case, we will look at sequences of words.\n",
        "In the 1D case, a filter is described as being size 3 if it looks at 3 words at a time.\n",
        "Just as the shape of 2D CNN filters actually has one more dimension, and therefore is a 3D tensor, the shape of 1D CNN filters is a 2D tensor.\n",
        "If the embedding representation for each word is 8 floating point numbers, then the size 3 filter will actually be a 3x8 tensor."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "JwIrBJBDXFo7"
      },
      "source": [
        "#### 1.2.1 Our vocabulary and embeddings\n",
        "\n",
        "We are going to create a small hand-crafted example to make it easy to see what is happening inside our models.\n",
        "\n",
        "Our vocabulary will consist of just seven words.  We will embed each word as a vector of eight numbers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Cr5IhRaTzVw"
      },
      "outputs": [],
      "source": [
        "# Our vocabulary\n",
        "the  = [0., 0., 0., 1., 2., 0., 1., 2.]\n",
        "boy  = [1., 0., 0., 0., 6., 0., 777., 888.]\n",
        "said = [0., 0., 1., 0., 0., 5., 5., 6.]\n",
        "he   = [0., 1., 0., 0., 0., 8., 33., 44.]\n",
        "was  = [0., 0., 1., 0., 7., 0., 9., 0.]\n",
        "good = [0., 0., 0., 1., 0., 3., 3., 4.]\n",
        "now  = [0., 0., 0., 1., 4., 4., 7., 8.]"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Wsx_y1TS8-PG"
      },
      "source": [
        "We will use pandas to nicely display all 48 floats in the representation of a six-word sentence:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "xmX4tElhX8V4",
        "outputId": "da467916-f021-441b-b82e-04b53e965b11"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-eb0f0ec9-e37a-48cc-b8a6-064601fdca03\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>the</th>\n",
              "      <th>boy</th>\n",
              "      <th>said</th>\n",
              "      <th>he</th>\n",
              "      <th>was</th>\n",
              "      <th>good</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>noun</th>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>pronoun</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>verb</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>other</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>val1</th>\n",
              "      <td>2.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>val2</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>other1</th>\n",
              "      <td>1.0</td>\n",
              "      <td>777.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>33.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>3.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>other2</th>\n",
              "      <td>2.0</td>\n",
              "      <td>888.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>44.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-eb0f0ec9-e37a-48cc-b8a6-064601fdca03')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-eb0f0ec9-e37a-48cc-b8a6-064601fdca03 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-eb0f0ec9-e37a-48cc-b8a6-064601fdca03');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "         the    boy  said    he  was  good\n",
              "noun     0.0    1.0   0.0   0.0  0.0   0.0\n",
              "pronoun  0.0    0.0   0.0   1.0  0.0   0.0\n",
              "verb     0.0    0.0   1.0   0.0  1.0   0.0\n",
              "other    1.0    0.0   0.0   0.0  0.0   1.0\n",
              "val1     2.0    6.0   0.0   0.0  7.0   0.0\n",
              "val2     0.0    0.0   5.0   8.0  0.0   3.0\n",
              "other1   1.0  777.0   5.0  33.0  9.0   3.0\n",
              "other2   2.0  888.0   6.0  44.0  0.0   4.0"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Visualizing a sequence of words\n",
        "seq1 = [the, boy, said, he, was, good]\n",
        "df = pd.DataFrame(seq1, columns=['noun', 'pronoun', 'verb', 'other', 'val1', 'val2', 'other1', 'other2'], \\\n",
        "                  index = ['the','boy','said','he','was','good'])\n",
        "df.T"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "5Lmjea7uYRoP"
      },
      "source": [
        "In our made-up example, we have assigned human-interpretable meanings to each number in the embedding.\n",
        "Real embeddings aren't likely to have values that are so easy to understand.\n",
        "We are using this kind of structure to make it easy to build examples that show what is happening in our neural networks."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "0Zrx3-z2gX7d"
      },
      "source": [
        "#### 1.2.2 Building a 1D CNN to detect where we have \"boy\" and \"he\" two words apart"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "L5KIuTda9qkP"
      },
      "source": [
        "We are going to build a simple 1D CNN with two size 3 filters.\n",
        "The first filter is going to detect the pronoun \"he\" that comes two words after the noun \"boy.\"\n",
        "In fact, if all nouns have a 1 in their first float and all pronouns have a 1 in their second float, then this filter will match every time any pronoun follows a noun two words later.\n",
        "In order to get a large output when we see this pattern, we are going to put 10's in our filter in the right places, and zeros everywhere else.\n",
        "We will need a 10 in the first position of the row corresponding to the first embedding float.\n",
        "We will also need a 10 in the third position of the row corresponding to the second embedding float.\n",
        "When the first word the filter is over is a noun and the third word the filter is over is a pronoun, the filter will output the value 20.\n",
        "For all other patterns, it will output a smaller number.\n",
        "\n",
        "So you can see other patterns in action, we will also create a second filter wich adds the `val1` value of the first word to the `val2` value of the third word.\n",
        "Here is the code:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3qSO0A9996Jp",
        "outputId": "7601e124-fcaa-4c83-9bc7-88673c706404"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Conv1d(8, 2, kernel_size=(3,), stride=(1,), padding=valid, bias=False)\n",
            "torch.Size([2, 8, 3])\n",
            "Parameter containing:\n",
            "tensor([[[10.,  0.,  0.],\n",
            "         [ 0.,  0., 10.],\n",
            "         [ 0.,  0.,  0.],\n",
            "         [ 0.,  0.,  0.],\n",
            "         [ 0.,  0.,  0.],\n",
            "         [ 0.,  0.,  0.],\n",
            "         [ 0.,  0.,  0.],\n",
            "         [ 0.,  0.,  0.]],\n",
            "\n",
            "        [[ 0.,  0.,  0.],\n",
            "         [ 0.,  0.,  0.],\n",
            "         [ 0.,  0.,  0.],\n",
            "         [ 0.,  0.,  0.],\n",
            "         [ 1.,  0.,  0.],\n",
            "         [ 0.,  0.,  1.],\n",
            "         [ 0.,  0.,  0.],\n",
            "         [ 0.,  0.,  0.]]], requires_grad=True)\n"
          ]
        }
      ],
      "source": [
        "# Instantiate a 1D CNN and manually set its weights\n",
        "conv = nn.Conv1d(in_channels=8, out_channels=2, kernel_size=(3,), stride=(1,), padding='valid', bias=False)\n",
        "\n",
        "wt = torch.tensor([[[10, 0, 0], [0, 0, 10], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0]], \\\n",
        "                   [[0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [1, 0, 0], [0, 0, 1], [0, 0, 0], [0, 0, 0.]]])\n",
        "conv.weight = nn.Parameter(wt)\n",
        "\n",
        "print(conv)\n",
        "print(conv.weight.shape)\n",
        "print(conv.weight)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "hjheANCqA10f"
      },
      "source": [
        "Now, let's send our six-word setence into our CNN and see the output.\n",
        "(Our CNN has padding turned off, so the output is going to be shorter than our input. In this case, it will be length 4.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bl-vIS219xTM",
        "outputId": "85b31bfd-2c42-4335-9bc0-ed7812345bd3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[ 0., 20.,  0.,  0.],\n",
            "        [ 7., 14.,  0.,  3.]], grad_fn=<SqueezeBackward1>)\n"
          ]
        }
      ],
      "source": [
        "input = torch.tensor(seq1).T\n",
        "out = conv(input)\n",
        "print(out)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "s_ionsK2e3lK"
      },
      "source": [
        "Our first filter is detecting a noun followed by a pronoun two words later.\n",
        "We see a big peak with the value 20 on the second word, \"boy.\"\n",
        "\n",
        "The second filter is summing the `val1` from the current word with the `val2` from two words later.  It outputs different values at each position.\n",
        "\n",
        "A subsequent layer could combine this information in interesting ways."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "7Mon4SIwgm8S"
      },
      "source": [
        "#### 1.2.3 A different pattern\n",
        "\n",
        "If we add the word \"now\" into the middle, the words \"boy\" and \"he\" are no longer two words apart."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "le-XTsaw9r_l",
        "outputId": "1287bb0f-8d89-4324-8673-31c45c1546e9"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-7966306f-1648-4340-ac06-38664c5046a1\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>the</th>\n",
              "      <th>boy</th>\n",
              "      <th>now</th>\n",
              "      <th>said</th>\n",
              "      <th>he</th>\n",
              "      <th>was</th>\n",
              "      <th>good</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>noun</th>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>pronoun</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>verb</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>other</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>val1</th>\n",
              "      <td>2.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>val2</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>other1</th>\n",
              "      <td>1.0</td>\n",
              "      <td>777.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>33.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>3.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>other2</th>\n",
              "      <td>2.0</td>\n",
              "      <td>888.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>44.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7966306f-1648-4340-ac06-38664c5046a1')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-7966306f-1648-4340-ac06-38664c5046a1 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-7966306f-1648-4340-ac06-38664c5046a1');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "         the    boy  now  said    he  was  good\n",
              "noun     0.0    1.0  0.0   0.0   0.0  0.0   0.0\n",
              "pronoun  0.0    0.0  0.0   0.0   1.0  0.0   0.0\n",
              "verb     0.0    0.0  0.0   1.0   0.0  1.0   0.0\n",
              "other    1.0    0.0  1.0   0.0   0.0  0.0   1.0\n",
              "val1     2.0    6.0  4.0   0.0   0.0  7.0   0.0\n",
              "val2     0.0    0.0  4.0   5.0   8.0  0.0   3.0\n",
              "other1   1.0  777.0  7.0   5.0  33.0  9.0   3.0\n",
              "other2   2.0  888.0  8.0   6.0  44.0  0.0   4.0"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "seq2 = [the, boy, now, said, he, was, good]\n",
        "df = pd.DataFrame(seq2, columns=['noun', 'pronoun', 'verb', 'other', 'val1', 'val2', 'other1', 'other2'], \\\n",
        "                  index = ['the','boy','now','said','he','was','good'])\n",
        "df.T"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ePXZBqFSBpCu"
      },
      "source": [
        "What happens when we pass this second sentence into our CNN?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kgN6ApIB9R0E",
        "outputId": "3c04c7c3-7495-4312-c6dc-8d1372bc750a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[ 0., 10., 10.,  0.,  0.],\n",
            "        [ 6., 11., 12.,  0.,  3.]], grad_fn=<SqueezeBackward1>)\n"
          ]
        }
      ],
      "source": [
        "input = torch.tensor(seq2).T\n",
        "out = conv(input)\n",
        "print(out)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "fApVwKVOhDG4"
      },
      "source": [
        "This sentence gets weak signals in certain places because we still have the words \"boy\" and \"he\" in the sentence,\n",
        "but we don't get the big peak (with value 20) anymore.\n",
        "\n",
        "We could add a new filter to detect nouns that are three words apart.\n",
        "Just like a 2D CNN used with images will have many filters to detect different patterns (such as vertical lines, horizontal lines, circles, etc.),\n",
        "a sophisticated 1D CNN used with sentences will need many filters to detect different patterns of words.\n",
        "\n",
        "Now that we have seen what pattern matching in 1D CNNs looks like, let's try detecting the same pattern using self-attention."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "PQ02h28GiP_i"
      },
      "source": [
        "## 2. Self-attention in transformers"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "vXLAmspMrt0H"
      },
      "source": [
        "<img src=\"https://lilianweng.github.io/posts/2018-06-24-attention/multi-head-attention.png\" width=250>"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "jVQ-v5ndiPyZ"
      },
      "source": [
        "### 2.1 Pattern matching with queries and keys"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "cv-R9CUDiZaZ"
      },
      "source": [
        "We will build our intuition about the power of self-attention by starting with just the query and key mechanism.\n",
        "Let's create Q and K weight matrices that will allow us to detect our words \"boy\" and \"he\" that are two words apart,\n",
        "just like we did with the 1D CNN.\n",
        "From this example, we can see how the dot product of queries and keys does matching.\n",
        "\n",
        "Because our word embedding size is 8, each of Q, K, and V are 8x8 tensors.\n",
        "The default PyTorch implementation stacks them together into a single 24x8 tensor.\n",
        "Recall that both the Query and Key matrices are going to be multiplied with our input tokens, creating length 8 vectors which we will dot product together.\n",
        "We are going to put a weight of 10 in the second column of the Query matrix so that it matches the 1 that pronouns have in the second embedding float.\n",
        "We will also put a weight of 10 in the first column of the Key matrix so it matches the 1 that nouns have in the first embedding float.\n",
        "If we put both 10's in the first rows, they will match up when we do the dot product.\n",
        "(We also have to put numbers in the weights for the Value matrix and the projection matrix that comes after self-attention, but will not focus on those weights yet.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "__KG9CfXePIE",
        "outputId": "b2d522b9-540e-4aaa-b576-478799aa565d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MultiheadAttention(\n",
            "  (out_proj): NonDynamicallyQuantizableLinear(in_features=8, out_features=8, bias=False)\n",
            ")\n",
            "Query weights: \n",
            " tensor([[ 0., 10.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]], grad_fn=<SliceBackward0>)\n",
            "Key weights: \n",
            " tensor([[10.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]], grad_fn=<SliceBackward0>)\n"
          ]
        }
      ],
      "source": [
        "# Instantiate an attention mechanism and manuall set its weights\n",
        "attn = nn.MultiheadAttention(embed_dim=8, num_heads=1, bias=False, batch_first=True)\n",
        "wt = torch.zeros(24,8)\n",
        "wt[0,1] = 10.\n",
        "wt[8,0] = 10.\n",
        "wt[16:24,0:8] = torch.eye(8)\n",
        "attn.in_proj_weight = nn.Parameter(wt)\n",
        "attn.out_proj.weight = nn.Parameter(torch.eye(8))\n",
        "\n",
        "print(attn)\n",
        "print(\"Query weights: \\n\", attn.in_proj_weight[0:8,:])\n",
        "print(\"Key weights: \\n\", attn.in_proj_weight[8:16,:])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "EMpOxjTEGiaK"
      },
      "source": [
        "When doing self-attention, the same data is used with the Q, K, and V weights.\n",
        "Let's load our original six-word sentence into variables and get ready to pass them into our self-attention module.\n",
        "Remember, \"he\" is the fourth word, and we want it to pay attention to \"boy,\" which is the second word."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "b88y-XafHlVK",
        "outputId": "46ea739b-288d-4b03-d399-3c5c6860f6fc"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-a7db02f1-212b-457f-839f-3d5e53114c2e\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>the</th>\n",
              "      <th>boy</th>\n",
              "      <th>said</th>\n",
              "      <th>he</th>\n",
              "      <th>was</th>\n",
              "      <th>good</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>noun</th>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>pronoun</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>verb</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>other</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>val1</th>\n",
              "      <td>2.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>val2</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>other1</th>\n",
              "      <td>1.0</td>\n",
              "      <td>777.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>33.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>3.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>other2</th>\n",
              "      <td>2.0</td>\n",
              "      <td>888.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>44.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a7db02f1-212b-457f-839f-3d5e53114c2e')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-a7db02f1-212b-457f-839f-3d5e53114c2e button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-a7db02f1-212b-457f-839f-3d5e53114c2e');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "         the    boy  said    he  was  good\n",
              "noun     0.0    1.0   0.0   0.0  0.0   0.0\n",
              "pronoun  0.0    0.0   0.0   1.0  0.0   0.0\n",
              "verb     0.0    0.0   1.0   0.0  1.0   0.0\n",
              "other    1.0    0.0   0.0   0.0  0.0   1.0\n",
              "val1     2.0    6.0   0.0   0.0  7.0   0.0\n",
              "val2     0.0    0.0   5.0   8.0  0.0   3.0\n",
              "other1   1.0  777.0   5.0  33.0  9.0   3.0\n",
              "other2   2.0  888.0   6.0  44.0  0.0   4.0"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df = pd.DataFrame(seq1, columns=['noun', 'pronoun', 'verb', 'other', 'val1', 'val2', 'other1', 'other2'], \\\n",
        "                  index = ['the','boy','said','he','was','good'])\n",
        "df.T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "04BVh1_nFSIE",
        "outputId": "3f91f5b1-db43-474d-9388-2d1cff0ac1cf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[  0.,   0.,   0.,   1.,   2.,   0.,   1.,   2.],\n",
            "        [  1.,   0.,   0.,   0.,   6.,   0., 777., 888.],\n",
            "        [  0.,   0.,   1.,   0.,   0.,   5.,   5.,   6.],\n",
            "        [  0.,   1.,   0.,   0.,   0.,   8.,  33.,  44.],\n",
            "        [  0.,   0.,   1.,   0.,   7.,   0.,   9.,   0.],\n",
            "        [  0.,   0.,   0.,   1.,   0.,   3.,   3.,   4.]])\n"
          ]
        }
      ],
      "source": [
        "q = torch.tensor(seq1)\n",
        "k = torch.tensor(seq1)\n",
        "v = torch.tensor(seq1)\n",
        "print(k)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "oHdYBtQ7HDNn"
      },
      "source": [
        "Now let's pass our input data in and see what the self-attention weights look like:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pNLRJxPoqQcK",
        "outputId": "39cc8ef6-b660-4c37-fd4c-bd8dadb860b8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[[0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667],\n",
            "         [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667],\n",
            "         [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667],\n",
            "         [0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667],\n",
            "         [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667]]],\n",
            "       grad_fn=<RoundBackward1>)\n",
            "Attention weights for the fourth word: \n",
            " tensor([0., 1., 0., 0., 0., 0.], grad_fn=<RoundBackward1>)\n"
          ]
        }
      ],
      "source": [
        "attn_output, attn_weights = attn(q, k , v, average_attn_weights=False)\n",
        "print(torch.round(attn_weights, decimals=4))\n",
        "print(\"Attention weights for the fourth word: \\n\", torch.round(attn_weights[0,3,:], decimals=4))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "7IpoBN6BHvRB"
      },
      "source": [
        "Success!  Our fourth word \"he\" has put 100% (when rounded) of it's attention on the word \"boy.\"\n",
        "For our simple hand-crafted attention weights, none of words other than \"he\" got any match with the queries and keys, so their attention is evenly spread across all six words.\n",
        "This might not be exactly the behavior we would want in a real language transformer, but remember this is a toy example designed to show how the attention mechanism works with real numbers."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "2RrFbnOsJJaI"
      },
      "source": [
        "Our 1D CNN filter, which matched when \"boy\" and \"he\" were two words apart, didn't work when they were three words apart.\n",
        "\n",
        "Can our self-attention do better?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z3qY4kYNLdbg",
        "outputId": "39155b2e-5e5a-4077-9680-5a10c1e13d5d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[  0.,   0.,   0.,   1.,   2.,   0.,   1.,   2.],\n",
            "        [  1.,   0.,   0.,   0.,   6.,   0., 777., 888.],\n",
            "        [  0.,   0.,   0.,   1.,   4.,   4.,   7.,   8.],\n",
            "        [  0.,   0.,   1.,   0.,   0.,   5.,   5.,   6.],\n",
            "        [  0.,   1.,   0.,   0.,   0.,   8.,  33.,  44.],\n",
            "        [  0.,   0.,   1.,   0.,   7.,   0.,   9.,   0.],\n",
            "        [  0.,   0.,   0.,   1.,   0.,   3.,   3.,   4.]])\n"
          ]
        }
      ],
      "source": [
        "q2 = torch.tensor(seq2)\n",
        "k2 = torch.tensor(seq2)\n",
        "v2 = torch.tensor(seq2)\n",
        "print(q2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A2T4Sd1QLHmw",
        "outputId": "dd07e0eb-fa28-4d22-c147-cf1eeb9520b2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[[0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429],\n",
            "         [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429],\n",
            "         [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429],\n",
            "         [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429],\n",
            "         [0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429],\n",
            "         [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429]]],\n",
            "       grad_fn=<RoundBackward1>)\n",
            "Attention weights for the fifth word: \n",
            " tensor([0., 1., 0., 0., 0., 0., 0.], grad_fn=<RoundBackward1>)\n"
          ]
        }
      ],
      "source": [
        "attn_output, attn_weights = attn(q2, k2 , v2, average_attn_weights=False)\n",
        "print(torch.round(attn_weights, decimals=4))\n",
        "print(\"Attention weights for the fifth word: \\n\", torch.round(attn_weights[0,4,:], decimals=4))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "NaNB9sSxJrlW"
      },
      "source": [
        "Yes!  Without changing our weights, our attention module successfully placed 100% of the attention for the word \"he\" (now the fifth word) onto the word \"boy.\"\n",
        "In fact, this set of weights would cause the word \"he\" to attend to the word \"boy\" no matter how far apart they are, or whether \"boy\" comes before or after the word \"he.\""
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ejPWpim7Kn5m"
      },
      "source": [
        "#### 2.2 Attention output"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Y7yliuHSK2hR"
      },
      "source": [
        "So far, we've only focused on what each word in our self-attention module is paying attention to.\n",
        "What is the output?\n",
        "\n",
        "When we created our attention module, in addition to setting the query and key weights, we set the value weight matrix to be the identity matrix.\n",
        "We also set the output projection to be the identity matrix.\n",
        "This means that the output will be a copy of the input, weighted by the attention scores.\n",
        "\n",
        "Let's pass our data in again, but this time examine our input and output tensors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TC7eGTLyMuCS",
        "outputId": "27399206-e73e-4136-910f-c527eddc43ac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input data: \n",
            " tensor([[  0.,   0.,   0.,   1.,   2.,   0.,   1.,   2.],\n",
            "        [  1.,   0.,   0.,   0.,   6.,   0., 777., 888.],\n",
            "        [  0.,   0.,   1.,   0.,   0.,   5.,   5.,   6.],\n",
            "        [  0.,   1.,   0.,   0.,   0.,   8.,  33.,  44.],\n",
            "        [  0.,   0.,   1.,   0.,   7.,   0.,   9.,   0.],\n",
            "        [  0.,   0.,   0.,   1.,   0.,   3.,   3.,   4.]])\n",
            "Output data: \n",
            " tensor([[  0.,   0.,   0.,   0.,   3.,   3., 119., 136.],\n",
            "        [  0.,   0.,   0.,   0.,   3.,   3., 119., 136.],\n",
            "        [  0.,   0.,   0.,   0.,   3.,   3., 119., 136.],\n",
            "        [  0.,   0.,   0.,   0.,   3.,   3., 119., 136.],\n",
            "        [  1.,   0.,   0.,   0.,   6.,   0., 777., 888.],\n",
            "        [  0.,   0.,   0.,   0.,   3.,   3., 119., 136.],\n",
            "        [  0.,   0.,   0.,   0.,   3.,   3., 119., 136.]],\n",
            "       grad_fn=<RoundBackward1>)\n",
            "Second word of the input: \n",
            " tensor([  1.,   0.,   0.,   0.,   6.,   0., 777., 888.])\n",
            "Fifth word of the output: \n",
            " tensor([  1.,   0.,   0.,   0.,   6.,   0., 777., 888.],\n",
            "       grad_fn=<RoundBackward1>)\n"
          ]
        }
      ],
      "source": [
        "attn_output, attn_weights = attn(q2, k2 , v2, average_attn_weights=False)\n",
        "print(\"Input data: \\n\", k)\n",
        "print(\"Output data: \\n\", torch.round(attn_output, decimals=0))\n",
        "print(\"Second word of the input: \\n\", torch.round(k[1,:], decimals=4))\n",
        "print(\"Fifth word of the output: \\n\", torch.round(attn_output[4,:], decimals=4))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "OQ-wPhpvNu5b"
      },
      "source": [
        "Our self-attention module copied the embedding for the word \"boy\" into the fifth location of the sequence where the word \"he\" is!\n",
        "A more complex version of this example is what allows transformer language models to copy or blend word embeddings based on what each word is attending to.\n",
        "If we use something other than an identity matrix for our value weights, we can also perform affine transformations on our input word embeddings."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Dk1LXLHb4b2N"
      },
      "source": [
        "### 2.3 Two attention heads"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ZkEAJOOhPCk9"
      },
      "source": [
        "For our final example, we are going to use multi-headed attention, and show two different operations happening at the same time.\n",
        "Remember that self-attention in a transformer is used in a residual network, which means the output is added to the input, then passed on to the next block."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "SOaPU_KsPCPO"
      },
      "source": [
        "We will instantiate a 2-headed attention module.\n",
        "The first attention head will copy the `other1` and `other2` from the word \"boy\" to the position of the word \"he.\"\n",
        "This is similar behavior to what we've already seen.\n",
        "The second attention head is going to attend the word \"he\" to itself.\n",
        "It is going to output -1 times its values for `other1` and `other2`, which means it is going to erase the current values so that the first attention head can copy the values from the word \"boy\" into empty (zero) embedding slots!\n",
        "\n",
        "The really interesting [Transformer Circuits work by Anthropic](https://transformer-circuits.pub/2021/framework/index.html) has seen this type of behavior.  They note, \"we've seen hints that some MLP neurons and attention heads may perform a kind of 'memory management' role, clearing residual stream dimensions set by other layers by reading in information and writing out the negative version.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f8XGAzE04na0",
        "outputId": "5f39cc84-4a4b-478f-f7ec-c84d31c2768e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MultiheadAttention(\n",
            "  (out_proj): NonDynamicallyQuantizableLinear(in_features=8, out_features=8, bias=False)\n",
            ")\n",
            "Q1:  tensor([[ 0., 10.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]], grad_fn=<SliceBackward0>)\n",
            "K1:  tensor([[10.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]], grad_fn=<SliceBackward0>)\n",
            "V1:  tensor([[0., 0., 0., 0., 1., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 1., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 1., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 1.]], grad_fn=<SliceBackward0>)\n",
            "Q2:  tensor([[ 0., 10.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]], grad_fn=<SliceBackward0>)\n",
            "K2:  tensor([[ 0., 10.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]], grad_fn=<SliceBackward0>)\n",
            "V2:  tensor([[0., 0., 0., 0., 1., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 1., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 1., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 1.]], grad_fn=<SliceBackward0>)\n",
            "Out1:  tensor([[0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0.]], grad_fn=<SliceBackward0>)\n",
            "Out2:  tensor([[ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "        [ 0.,  0.,  1.,  0.,  0.,  0., -1., -0.],\n",
            "        [ 0.,  0.,  0.,  1.,  0.,  0., -0., -1.]], grad_fn=<SliceBackward0>)\n"
          ]
        }
      ],
      "source": [
        "attn2 = nn.MultiheadAttention(embed_dim=8, num_heads=2, bias=False, batch_first=True)\n",
        "\n",
        "wt = torch.zeros(24,8)\n",
        "wt[0,1] = 10.\n",
        "wt[8,0] = 10.\n",
        "wt[4,1] = 10.\n",
        "wt[12,1] = 10.\n",
        "wt[16:20,4:8] = torch.eye(4)\n",
        "wt[20:24,4:8] = torch.eye(4)\n",
        "attn2.in_proj_weight = nn.Parameter(wt)\n",
        "\n",
        "out_wt = torch.zeros(8,8)\n",
        "out_wt[6:8,2:4] = torch.eye(2)\n",
        "out_wt[6:8,6:8] = -1. * torch.eye(2)\n",
        "attn2.out_proj.weight = nn.Parameter(out_wt)\n",
        "\n",
        "print(attn)\n",
        "# print(attn2.in_proj_weight)\n",
        "print(\"Q1: \",attn2.in_proj_weight[0:4,:])\n",
        "print(\"K1: \",attn2.in_proj_weight[8:12,:])\n",
        "print(\"V1: \",attn2.in_proj_weight[16:20,:])\n",
        "print(\"Q2: \",attn2.in_proj_weight[4:8,:])\n",
        "print(\"K2: \",attn2.in_proj_weight[12:16,:])\n",
        "print(\"V2: \",attn2.in_proj_weight[20:24,:])\n",
        "\n",
        "print(\"Out1: \",attn2.out_proj.weight[0:4,:])\n",
        "print(\"Out2: \",attn2.out_proj.weight[4:8,:])\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "wpccEbR2fAO4"
      },
      "source": [
        "Here are the attention values when we input our sentence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-FKDn8VQ4ruC",
        "outputId": "d52d0164-8c97-427e-8e4e-b3bf81620d21"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[[0.1430, 0.1430, 0.1430, 0.1430, 0.1430, 0.1430, 0.1430],\n",
            "         [0.1430, 0.1430, 0.1430, 0.1430, 0.1430, 0.1430, 0.1430],\n",
            "         [0.1430, 0.1430, 0.1430, 0.1430, 0.1430, 0.1430, 0.1430],\n",
            "         [0.1430, 0.1430, 0.1430, 0.1430, 0.1430, 0.1430, 0.1430],\n",
            "         [0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.1430, 0.1430, 0.1430, 0.1430, 0.1430, 0.1430, 0.1430],\n",
            "         [0.1430, 0.1430, 0.1430, 0.1430, 0.1430, 0.1430, 0.1430]],\n",
            "\n",
            "        [[0.1430, 0.1430, 0.1430, 0.1430, 0.1430, 0.1430, 0.1430],\n",
            "         [0.1430, 0.1430, 0.1430, 0.1430, 0.1430, 0.1430, 0.1430],\n",
            "         [0.1430, 0.1430, 0.1430, 0.1430, 0.1430, 0.1430, 0.1430],\n",
            "         [0.1430, 0.1430, 0.1430, 0.1430, 0.1430, 0.1430, 0.1430],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000],\n",
            "         [0.1430, 0.1430, 0.1430, 0.1430, 0.1430, 0.1430, 0.1430],\n",
            "         [0.1430, 0.1430, 0.1430, 0.1430, 0.1430, 0.1430, 0.1430]]],\n",
            "       grad_fn=<RoundBackward1>)\n"
          ]
        }
      ],
      "source": [
        "attn_output, attn_weights = attn2(q2, k2 , v2, average_attn_weights=False)\n",
        "print(torch.round(attn_weights, decimals=3))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "n-ntxUpBzih5"
      },
      "source": [
        "In the first attention head, the word \"he\" is putting 100% attention on \"boy.\"\n",
        "In the second attention head, the word \"he\" is 100% attending to itself."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Z_gKVT0MjmR7"
      },
      "source": [
        "And now we show the input, the raw output, and the result of adding the output to the input."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lm6ByZY_5Lee",
        "outputId": "5b199661-27bd-4bc9-a631-51a467f97f30"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input:\n",
            "tensor([[  0.,   0.,   0.,   1.,   2.,   0.,   1.,   2.],\n",
            "        [  1.,   0.,   0.,   0.,   6.,   0., 777., 888.],\n",
            "        [  0.,   0.,   0.,   1.,   4.,   4.,   7.,   8.],\n",
            "        [  0.,   0.,   1.,   0.,   0.,   5.,   5.,   6.],\n",
            "        [  0.,   1.,   0.,   0.,   0.,   8.,  33.,  44.],\n",
            "        [  0.,   0.,   1.,   0.,   7.,   0.,   9.,   0.],\n",
            "        [  0.,   0.,   0.,   1.,   0.,   3.,   3.,   4.]])\n",
            "Output:\n",
            "tensor([[  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
            "        [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
            "        [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
            "        [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
            "        [  0.,   0.,   0.,   0.,   0.,   0., 744., 844.],\n",
            "        [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
            "        [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.]],\n",
            "       grad_fn=<RoundBackward1>)\n",
            "Output summed with input:\n",
            "tensor([[  0.,   0.,   0.,   1.,   2.,   0.,   1.,   2.],\n",
            "        [  1.,   0.,   0.,   0.,   6.,   0., 777., 888.],\n",
            "        [  0.,   0.,   0.,   1.,   4.,   4.,   7.,   8.],\n",
            "        [  0.,   0.,   1.,   0.,   0.,   5.,   5.,   6.],\n",
            "        [  0.,   1.,   0.,   0.,   0.,   8., 777., 888.],\n",
            "        [  0.,   0.,   1.,   0.,   7.,   0.,   9.,   0.],\n",
            "        [  0.,   0.,   0.,   1.,   0.,   3.,   3.,   4.]],\n",
            "       grad_fn=<RoundBackward1>)\n"
          ]
        }
      ],
      "source": [
        "print(\"Input:\")\n",
        "print(torch.round(k2, decimals=4))\n",
        "print(\"Output:\")\n",
        "print(torch.round(attn_output, decimals=4))\n",
        "print(\"Output summed with input:\")\n",
        "print(torch.round(k2 + attn_output, decimals=4))\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "8BD7TfL4la_-"
      },
      "source": [
        "The combination of our two attention heads has cleared the values that the fifth word \"he\" had in the 7th and 8th embedding floats and copied the values from the word \"boy\" into those spots."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "F7I0pDLNnyP5"
      },
      "source": [
        "To see this one head at a time, we can zero out the output from one head, then the other, and finally sum them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fEgrTgOSnxNo",
        "outputId": "dbb6cfd0-3192-4e4a-c502-62563519b2ef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[  0.,   0.,   0.,   0.,   0.,   0., 119., 136.],\n",
            "        [  0.,   0.,   0.,   0.,   0.,   0., 119., 136.],\n",
            "        [  0.,   0.,   0.,   0.,   0.,   0., 119., 136.],\n",
            "        [  0.,   0.,   0.,   0.,   0.,   0., 119., 136.],\n",
            "        [  0.,   0.,   0.,   0.,   0.,   0., 777., 888.],\n",
            "        [  0.,   0.,   0.,   0.,   0.,   0., 119., 136.],\n",
            "        [  0.,   0.,   0.,   0.,   0.,   0., 119., 136.]],\n",
            "       grad_fn=<RoundBackward1>)\n"
          ]
        }
      ],
      "source": [
        "out_wt = torch.zeros(8,8)\n",
        "out_wt[6:8,2:4] = torch.eye(2)\n",
        "# out_wt[6:8,6:8] = -1. * torch.eye(2)      # zeroed output of second attention head\n",
        "attn2.out_proj.weight = nn.Parameter(out_wt)\n",
        "\n",
        "attn_output1, attn_weights1 = attn2(q2, k2 , v2, average_attn_weights=False)\n",
        "\n",
        "print(torch.round(attn_output1, decimals=0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4eyZ8JKVot_e",
        "outputId": "523f43a8-f803-417a-c041-f3598cbf545a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[   0.,    0.,    0.,    0.,    0.,    0., -119., -136.],\n",
            "        [   0.,    0.,    0.,    0.,    0.,    0., -119., -136.],\n",
            "        [   0.,    0.,    0.,    0.,    0.,    0., -119., -136.],\n",
            "        [   0.,    0.,    0.,    0.,    0.,    0., -119., -136.],\n",
            "        [   0.,    0.,    0.,    0.,    0.,    0.,  -33.,  -44.],\n",
            "        [   0.,    0.,    0.,    0.,    0.,    0., -119., -136.],\n",
            "        [   0.,    0.,    0.,    0.,    0.,    0., -119., -136.]],\n",
            "       grad_fn=<RoundBackward1>)\n"
          ]
        }
      ],
      "source": [
        "out_wt = torch.zeros(8,8)\n",
        "# out_wt[6:8,2:4] = torch.eye(2)      # zeroed output of first attention head\n",
        "out_wt[6:8,6:8] = -1. * torch.eye(2)\n",
        "attn2.out_proj.weight = nn.Parameter(out_wt)\n",
        "\n",
        "attn_output2, attn_weights2 = attn2(q2, k2 , v2, average_attn_weights=False)\n",
        "\n",
        "print(torch.round(attn_output2, decimals=0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vkr7OIq-o66e",
        "outputId": "d69c53c1-15b3-40aa-fdec-8d717393beb5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
            "        [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
            "        [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
            "        [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
            "        [  0.,   0.,   0.,   0.,   0.,   0., 744., 844.],\n",
            "        [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
            "        [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.]],\n",
            "       grad_fn=<AddBackward0>)\n",
            "tensor([[  0.,   0.,   0.,   1.,   2.,   0.,   1.,   2.],\n",
            "        [  1.,   0.,   0.,   0.,   6.,   0., 777., 888.],\n",
            "        [  0.,   0.,   0.,   1.,   4.,   4.,   7.,   8.],\n",
            "        [  0.,   0.,   1.,   0.,   0.,   5.,   5.,   6.],\n",
            "        [  0.,   1.,   0.,   0.,   0.,   8., 777., 888.],\n",
            "        [  0.,   0.,   1.,   0.,   7.,   0.,   9.,   0.],\n",
            "        [  0.,   0.,   0.,   1.,   0.,   3.,   3.,   4.]],\n",
            "       grad_fn=<RoundBackward1>)\n"
          ]
        }
      ],
      "source": [
        "print(attn_output1 + attn_output2)\n",
        "print(torch.round(k2 + attn_output1 + attn_output2, decimals=4))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "779vT30CpGgH"
      },
      "source": [
        "The values for the two attention heads are negatives of each other for every word except the fifth word.\n",
        "For our critical fifth word, the second attention head outputs the negative of the current value for the seventh and eighth embedding floats, zeroing them out.\n",
        "The first attention head copies the seventh and eight embedding floats from the word \"boy.\"\n",
        "We could have just as easily copied different embedding values into the seventh and eighth positions and done some linear transformation too.\n",
        "\n",
        "Feel free to play around with the weights and see what kinds of behaviors you can create."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "0lN8bXGol2xs"
      },
      "source": [
        "## 3. Conclusion"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "pv7ALUNJkDLj"
      },
      "source": [
        "We hope these examples have given you some additional intuition why self-attention seems to be so powerful.\n",
        "\n",
        "* One strength is that rules embedded into the Query and Key weights can work based on the meaning of the words as encoded into the word embeddings, and these rules will work regardless of where the words appear.\n",
        "* If we want the model to care about the absolute or relative position of words, the queries and keys can also utilize the positional embedding data.\n",
        "* In addition to calculating attention scores for which input(s) in the sequence to pay attention to, the Value weights allow the attention mechanism to transform or embed portions of the input, making more interesting data manipulations possible.\n",
        "* Multi-headed attention increases the number of rules one attention unit can perform without increasing the computation cost (when implemented with the default hidden size that inversely scales with the number of heads).\n",
        "* Finally, because self-attention is used in ResNet fashion, the attention mechanisms can copy values, blend values, and even clear values in the residual stream."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "qtlOvlBnDbUC"
      },
      "source": [
        "While the examples provided here were hand-crafted with interpretable word embeddings that had orthogonal meanings, such as the first float indicating whether the word was a noun or not, this is not a completely far fetched scenario.\n",
        "Large language models often use embedding spaces that are size 1024 or even 2048.\n",
        "In such high dimensional vector spaces, many vectors will be orthogonal -- or very close to orthogonal.\n",
        "It seems intuitive that during training the transformer should have little difficulty finding orthogonal reqpresentations for pattern matching, if that is desired."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kYmiS-HmChs4"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
